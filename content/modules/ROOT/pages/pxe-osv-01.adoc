== Scenario - Storage Classes and Storage Profiles

In this scerario, we will be install Portworx Enterprise on an existing openshift cluster, as well as creating a new default StorageClass for virtual machines.

=== Creating our project

For this lab, we will be using a new project. In the terminal window execute the following command to create a new project.

[source,sh,role=execute]
----
oc new-project vmtest
----

=== Reminder: Accessing the Openshift Console

You can connect to the terminal in the windows to the right.

====
[TIP]

The {openshift_cluster_console_url}[OpenShift Web Console^] tab will open in a new browser.
window.

The username is `{openshift_cluster_admin_username}` and the password is `{openshift_cluster_admin_password}`
====

=== Task: Exploring the CLI

Red Hat OpenShift uses the `oc` cli utility. This utility has a similar
syntax to kubectl, but with some OpenShift specific extensions.

Let’s look at the nodes that make up our cluster:

[source,sh,role=execute]
----
oc get nodes
----

== Install Portworx

To install Portworx, we first need to install the Portworx Operator.

=== Task: Install the Portworx Operator

Navigate to `Operators` > `Operator Hub` and type `Portworx` in to the
filter as seen in this screenshot:

.install-operator
image:02-pxeinstall-installoperator-01.png[install-operator]

and then click on the `Portworx Enterprise` operator.

We will now see the installation screen on the right of our interface.
The defaults will suffice, and will grab the latest stable version of
the operator. Click `Install`

.install-operator02
image:03-pxeinstall-installoperator-02.png[install-operator02]

On the next screen, `enable` the `Console plugin`

Create the a new project named `portworx` and install the Portworx
operator in to the new `portworx` project.

.install-operator03
image:04-pxeinstall-installoperator-03.png[install-operator03]

and click the `Install` button.

The installation will prompt you to create a `StorageCluster` object,
instead, click `View installed Operators in Namespace portworx`

=== Task: Install Portworx StorageCluster

For this step, we need to switch back to our `Terminal` tab in the
Instruqt interface.

Installing the the Portworx StorageCluster requires a few steps. We need
to ensure that we have a service account secret for our gcloud
environment to create and manage disks. Portworx will create and expand
GCP disks based on our specifications.

Create this secret by running:

[source,sh,role=execute]
----
echo $INSTRUQT_GCP_PROJECT_GCPPROJECT_SERVICE_ACCOUNT_KEY | base64 -d > gcloud.json
oc -n portworx create secret generic px-gcloud --from-file=gcloud.json
----

We can now grab our StorageCluster specification:

[source,sh,role=execute]
----
curl -o px-spec.yaml "https://install.portworx.com/3.1?operator=true&mc=false&kbver=1.29.8&ns=portworx&b=true&iop=6&s=%22type%3Dpd-standard%2Csize%3D50%22&ce=gce&r=17001&c=px-cluster&osft=true&stork=true&csi=true&mon=true&tel=false&st=k8s&promop=true"
----

We now need to insert a reference for our secret in to our
StorageCluster specification. This can be done with a little `yq` magic:

[source,sh,role=execute]
----
yq -iy '.spec.volumes += [{"name": "gcloud", "mountPath": "/etc/pwx/gce", "secret": {"secretName": "px-gcloud"}}] | .spec.env += [{"name": "GOOGLE_APPLICATION_CREDENTIALS", "value": "/etc/pwx/gce/gcloud.json"}]' px-spec.yaml
----

Take a look at our current manifest by using the cat utility:

[source,sh,role=execute]
----
cat px-spec.yaml
----

* Lines 14-16 contain our disk configuration. These disks will be
created and attached as part of the Portworx installation.
* Lines 35-37 contain a reference to our OpenShift secret that we will
use to provision the above disk specification.

Additional documentation can be found
https://docs.portworx.com/portworx-enterprise/platform/openshift/ocp-gcp/install-on-ocp-gcp[here]

We can now apply the specification:

[source,sh,role=execute]
----
oc apply -f px-spec.yaml
----

The install can take about 5 minutes. We can watch the containers come
up by running:

[source,sh,role=execute]
----
watch oc -n portworx get pods
----

When all three of the `px-cluster` pods have a Ready status of `1/1` we
can press `ctrl-c` to exit out of our watch command.

=== Task: Check the Portworx cluster status

Portworx ships with a `pxctl` CLI utility that you can use for managing
Portworx. We’ll cover this utility more in the labs here in a little
bit!

For now, we’ll run `sudo pxctl status` via `oc` in one of the Portworx
pods to check the StorageCluster status.

First, setup the `PX_POD` environment variable:

[source,sh,role=execute]
----
PX_POD=$(oc get pods -l name=portworx -n portworx -o jsonpath='{.items[0].metadata.name}')
----

Next, use `oc` to execute `sudo pxctl status` within a pod defined by
the `PX_POD` environment variable to check the Portworx StorageCluster
status:

[source,sh,role=execute]
----
oc exec -it $PX_POD -n portworx -- /opt/pwx/bin/pxctl status --color
----

We now have a 3-node Portworx cluster up and running!

Let’s dive into our cluster status: - All 3 nodes are online and use
Kubernetes node names as the Portworx node IDs.

* Portworx detected the block device media type as
``STORAGE_MEDIUM_MAGNETIC'', and created a storage pool for those disks.
If you have different types of disk, for example SSD and
magnetic/rotational disk, a dedicated storage pool would be created for
each type of device.

To make things easier throughout the lab, let’s set a bash alias for
pxctl:

[source,sh,role=execute]
----
echo "alias pxctl='PX_POD=\$(oc get pods -l name=portworx -n portworx --field-selector=status.phase==Running | grep \"1/1\" | awk \"NR==1{print \$1}\") && oc exec \$PX_POD -n portworx -- /opt/pwx/bin/pxctl'" >> /root/.profile
source /root/.profile
----

Now test out the alias:

[source,sh,role=execute]
----
pxctl status --color
----


== Storage Classes and Storage Profiles in Openshift

Storage Classes are a Kubernetes concept that allows an administrator
to describe _classes_ of storage they offer. Storage Classes are
unopinionated about what the class represents, but it may include things
such as: quality-of-service levels, backup policies, or snapshot
policies.

Portworx storage classes offer a number of configuration parameters that
can be used to configure the amount of replicas, or encryption-at-rest
configurations.

Storage Classes are not specific to Openshift or Virtualization, but we
still need a storage class to provision virtual machine disks.

=== Task 1: View existing storage classes

Portworx deploys serveral pre-configured storage classes when the
storage cluster was created. These storage classes offer a veriety of
configuration options. To view the current storage classes run:

[source,sh,role=execute]
----
oc get sc
----

Portworx offers Kubernetes in-tree and CSI provisioners. Storage Classes
that contain the `-csi-` string.

Let's look at the configuration of an example storage class:

[source,sh,role=execute]
----
oc get sc px-csi-db -o yaml
----

We can see in the terminal output a list of parameters. This isn’t
exactly what we want for our new virtual machines, so let’s create a new
storage class.

=== Task 2: Create a new storage class for VMs

Run the following command to create a new yaml file for the block-based
StorageClass configuration:

[source,sh,role=execute]
----
cat << EOF |oc apply -f -
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: px-csi-vm
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
parameters:
  repl: "3"
  sharedv4: "true"
  sharedv4_svc_type: "ClusterIP"
  sharedv4_mount_options: vers=3.0,nolock
provisioner: pxd.portworx.com
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
EOF
----

PVCs provisioned using the above StorageClass will have a replication
factor of 3, which means there will be three replicas of the PVC spread
across the OpenShift worker nodes.

We have also set some configuration options on how RWX volumes should
work. We specified the service type to `ClusterIP` which uses a cluster
IP as the endpoint of NFS, and set some mount options.

We also specified that the volumeBindingMode should be
`WaitForFirstConsumer` to allow Portworx to intelligently place the
volume.

See the
https://docs.portworx.com/portworx-enterprise/3.1/platform/openshift/ocp-bare-metal/operations/storage-operations/manage-kubevirt-vms)[Portworx
Documentation^] for further details.

Also note that the `provisioner` is set to `pxd.portworx.com`. This
means that our storage class will be using CSI rather than the in-tree
provisioner.

With our StorageClass created, we can now create move on to Storage
Profiles.

=== Task 3: Configure the Storage Profile

Storage Profiles provide recommended storage settings based on an
associate storage class. Storage profiles are automatically created in
Openshift when a new storage class is created.

Portworx sets desired parameters when using the CSI provider, including
the prefered access mode.

We can see the current configuration of our new storage profile by
running:

[source,sh,role=execute]
----
oc get storageprofile px-csi-vm-example -o yaml
----

We can see under the `.status` node a list of access modes. The first
access mode: RWX in filesystem mode will be prefered.

For further details on storage clusters, see the
https://docs.openshift.com/container-platform/4.16/virt/storage/virt-configuring-storage-profile.html)[Openshift
documentation^].

We can now create virtual machines using our new storage profile!
